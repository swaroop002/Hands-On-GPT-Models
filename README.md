# Hands-On-GPT-Models
The project encompasses a series of interactive code segments implemented in a Jupyter Notebook (ipynb file). Each section focuses on a specific NLP task, ranging from text generation and sentiment analysis to text summarization, named entity recognition (NER), and question answering. The project utilizes pre-trained models such as GPT-2, DistilBERT, DistilBART, and BERT, all available within the Hugging Face Transformers ecosystem.

## Models:

GPT-2 (Generative Pre-trained Transformer 2):
<pre>
Task: Text Generation
Pipeline: text-generation
Example: Generating creative text based on given prompts.
</pre>
DistilBERT:
<pre>
Task: Sentiment Analysis
Pipeline: sentiment-analysis
Example: Analyzing sentiment in a set of sample sentences.
</pre>
DistilBART (Distilled BART):
<pre>
Task: Text Summarization
Pipeline: summarization
Example: Generating a concise summary for a given article.
</pre>
BERT (Bidirectional Encoder Representations from Transformers):
<pre>
Task: Named Entity Recognition (NER)
Pipeline: ner
Example: Identifying named entities in a text.
</pre>
DistilBERT:
<pre>
Task: Question Answering
Pipeline: question-answering
Example: Answering questions based on a provided context.
</pre>
